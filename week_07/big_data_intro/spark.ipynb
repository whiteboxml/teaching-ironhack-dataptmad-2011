{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right; padding-top: 15px; padding-right: 15px\">\n",
    "    <div>\n",
    "        <a href=\"https://whiteboxml.com\">\n",
    "            <img src=\"https://whiteboxml.com/static/img/logo/black_bg_white.svg\" width=\"250\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When dataset size exceeds your computer memory (RAM or even storage), [Big Data](https://en.wikipedia.org/wiki/Big_data) tools are used to break the dataset in chunks and process it step by step\n",
    "* Big Data tools allow you to make this process automatically and take care of everything under the hood with little extra code\n",
    "* [Spark](https://spark.apache.org/) is the most popular Big Data framework so far\n",
    "* Spark syntax resembles pandas API with some differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need Java to run Spark. It is written in [Scala](https://www.scala-lang.org/), a JVM based (and functional style) language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Java installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda install openjdk -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Apt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sudo apt install default-jdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PySpark installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda install pyspark -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('big_data_session') \\\n",
    "            .master('local[*]') \\\n",
    "            .config('spark.ui.showConsoleProgress', True) \\\n",
    "            .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "            .config('spark.sql.session.timeZone', 'UTC') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data used in this workshop will be downloaded from [datamarket.es](https://datamarket.es/), the reference website for retrieving external data in Spain. Two sources has been sampled:\n",
    "\n",
    "- Renfe trips\n",
    "- Supermarket products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Renfe trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/ubuntu/Desktop/renfe.csv'\n",
    "\n",
    "sdf = spark.read.option('quote', '\"').option('escape', '\\\\').csv(DATA_PATH, \n",
    "                                                                 header=True, \n",
    "                                                                 inferSchema=True)\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__VERY IMPORTANT INFO: sdf is a Spark DataFrame, which means it is a distributed DataFrame, not a typical Python object that lives in RAM memory (Pandas DataFrame)__\n",
    "\n",
    "- From DataBricks (Spark creators) about what a Spark DataFrame is:\n",
    "\n",
    "_\"In Spark, a DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a dataframe in Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs (Resilient Distributed Datasets)\"_\n",
    "\n",
    "- Spark DataFrames do no live in computers / cluster nodes memory, they are evaluated at the time some calculations are required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "date_cols_meta = ['departure', 'arrival', 'insert_date']\n",
    "\n",
    "for dt_col in date_cols_meta:\n",
    "    sdf = sdf.withColumn(dt_col, sf.to_timestamp(dt_col))\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sample = sdf.sample(fraction=0.1, withReplacement=False)\n",
    "\n",
    "sdf_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### persist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH = '/home/ubuntu/Desktop/renfe_sample'\n",
    "\n",
    "sdf_sample.write.mode('overwrite').parquet(SAMPLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sample = spark.read.parquet(SAMPLE_PATH)\n",
    "\n",
    "sdf_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sample.select(['origin', 'destination']).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_filter = sf.col('meta') != '{}'\n",
    "duration_filter = sf.col('duration') < 4.0\n",
    "seats_filter = sf.col('seats').isNotNull()\n",
    "\n",
    "sdf_filtered = sdf_sample.filter(meta_filter & duration_filter & seats_filter)\n",
    "\n",
    "sdf_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sdf_filtered.withColumn('duration_computed', (sf.col('arrival').cast(IntegerType()) - sf.col('departure').cast(IntegerType())) / 3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_filtered.groupby(['origin', 'destination']).agg({'price': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_filtered.select(['meta']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@sf.udf('integer')\n",
    "def get_first_class_first_fare_seats(meta):\n",
    "    try:\n",
    "        meta_dict = eval(meta)\n",
    "        first_available_class = [*meta_dict][0]\n",
    "        first_available_fare = [*meta_dict[first_available_class]][0]\n",
    "        seats = meta_dict[first_available_class][first_available_fare]['seats']\n",
    "        return seats\n",
    "\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_filtered = sdf_filtered.withColumn('seats_first_class_first_fare', get_first_class_first_fare_seats(sf.col('meta')))\n",
    "\n",
    "sdf_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_filtered = sdf_filtered.withColumnRenamed('seats', 'seats_cheapest_class_cheapest_fare')\n",
    "\n",
    "sdf_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create virtual sql tables and query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_filtered.createTempView('renfe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY = \"\"\"\n",
    "select\n",
    "origin,\n",
    "destination,\n",
    "avg(price) as mean_price, \n",
    "avg(seats_cheapest_class_cheapest_fare) as mean_seats\n",
    "from renfe\n",
    "group by origin, destination\n",
    "order by mean_price desc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_prices_sdf = spark.sql(SQL_QUERY)\n",
    "\n",
    "routes_prices_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform Spark DataFrame into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_prices_df = routes_prices_sdf.toPandas()\n",
    "\n",
    "routes_prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Supermarket products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/ubuntu/Desktop/supermarkets.csv'\n",
    "\n",
    "sdf = spark.read.csv(DATA_PATH, \n",
    "                     header=True, \n",
    "                     inferSchema=True)\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = 'insert_date'\n",
    "\n",
    "sdf = sdf.withColumn('year', sf.year(date_col))\n",
    "sdf = sdf.withColumn('month', sf.month(date_col))\n",
    "sdf = sdf.withColumn('day', sf.dayofmonth(date_col))\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/home/ubuntu/Desktop/supermarkets_partitioned'\n",
    "\n",
    "sdf.write.partitionBy('year', 'month', 'day').mode('overwrite').parquet(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('supermarket', 'name').distinct().groupby('supermarket').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(sf.min(date_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(sf.max(date_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_insert_date = sdf.select(sf.max(date_col).alias('max_insert_date')).collect()[0]['max_insert_date']\n",
    "\n",
    "max_insert_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.filter(sf.col(date_col) == max_insert_date) \\\n",
    "  .groupby('supermarket', 'category') \\\n",
    "  .agg(sf.min('price') \\\n",
    "  .alias('min_price')) \\\n",
    "  .sort('min_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_filter = sf.col('supermarket') == 'mercadona-es'\n",
    "category_filter = sf.col('category') == 'panaderia_y_pasteleria_pan_de_horno'\n",
    "date_filter = sf.col(date_col) == max_insert_date\n",
    "\n",
    "sdf.filter(supermarket_filter & category_filter & date_filter).sort('price', sf.col('reference_price').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding-top: 25px; float: right\">\n",
    "    <div>    \n",
    "        <i>&nbsp;&nbsp;Â© Copyright by</i>\n",
    "    </div>\n",
    "    <div>\n",
    "        <a href=\"https://whiteboxml.com\">\n",
    "            <img src=\"https://whiteboxml.com/static/img/logo/black_bg_white.svg\" width=\"125\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark_env]",
   "language": "python",
   "name": "conda-env-spark_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
